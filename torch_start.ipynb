{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_start.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaG9dU4HvRYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYyeSjDgvfJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "164b0eb7-19d1-4612-f02b-918b8387a326"
      },
      "source": [
        "x_data = [1, 2, 3]\n",
        "y_data = [2, 4, 6]\n",
        "\n",
        "w = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
        "\n",
        "def forward(x):\n",
        "    return x*w \n",
        "\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y)**2\n",
        "print('prediction before training', 4, forward(4).data[0])\n",
        "\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        l = loss(x_val, y_val)\n",
        "        l.backward()\n",
        "        print('\\tgrad: ', x_val, y_val, w.grad.data[0])\n",
        "        w.data = w.data - 0.01*w.grad.data\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction before training 4 tensor(4.)\n",
            "\tgrad:  1 2 tensor(-2.)\n",
            "\tgrad:  2 4 tensor(-9.8400)\n",
            "\tgrad:  3 6 tensor(-25.7088)\n",
            "\tgrad:  1 2 tensor(-26.9578)\n",
            "\tgrad:  2 4 tensor(-29.7973)\n",
            "\tgrad:  3 6 tensor(-30.8226)\n",
            "\tgrad:  1 2 tensor(-30.3201)\n",
            "\tgrad:  2 4 tensor(-25.8843)\n",
            "\tgrad:  3 6 tensor(-11.2448)\n",
            "\tgrad:  1 2 tensor(-9.3933)\n",
            "\tgrad:  2 4 tensor(-1.2357)\n",
            "\tgrad:  3 6 tensor(17.3411)\n",
            "\tgrad:  1 2 tensor(19.0584)\n",
            "\tgrad:  2 4 tensor(24.4028)\n",
            "\tgrad:  3 6 tensor(32.0352)\n",
            "\tgrad:  1 2 tensor(32.2426)\n",
            "\tgrad:  2 4 tensor(30.4925)\n",
            "\tgrad:  3 6 tensor(21.0663)\n",
            "\tgrad:  1 2 tensor(19.5976)\n",
            "\tgrad:  2 4 tensor(12.1551)\n",
            "\tgrad:  3 6 tensor(-6.7786)\n",
            "\tgrad:  1 2 tensor(-8.7467)\n",
            "\tgrad:  2 4 tensor(-15.9197)\n",
            "\tgrad:  3 6 tensor(-29.1932)\n",
            "\tgrad:  1 2 tensor(-30.0842)\n",
            "\tgrad:  2 4 tensor(-31.2414)\n",
            "\tgrad:  3 6 tensor(-28.2215)\n",
            "\tgrad:  1 2 tensor(-27.3216)\n",
            "\tgrad:  2 4 tensor(-21.5360)\n",
            "\tgrad:  3 6 tensor(-4.6419)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}